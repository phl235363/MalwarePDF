from multiprocessing import freeze_support
import numpy  as np
import pandas as  pd 

import os
#In [5]
from sklearn.tree import DecisionTreeClassifier, export_graphviz
from sklearn.ensemble import AdaBoostClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score, recall_score, precision_score, f1_score, accuracy_score, roc_curve
from fastcore.basics import *
from fastcore.parallel import *
from os import cpu_count
from sklearn.metrics import roc_auc_score
#In [20]
import seaborn as sns
import matplotlib.pyplot as plt











if __name__ == '__main__':
    freeze_support()

    current_dir = os.path.dirname(os.path.abspath(__file__))
    file_path=current_dir+"/PDFMalware2022.parquet"
    df = pd.read_parquet(file_path)
    #In [2]
    print("#out [2]")
    print(df.shape)
    #In [3]
    print("#Out [3]")
    print(df.loc[df.select_dtypes(include='category').apply(lambda x: x.str.contains('\(|\)|>', regex=True)).any(axis=1)]['Class'].value_counts())
   
    #In [4]
    print("#Out [4]")
    print(df.loc[df.select_dtypes(include='category').apply(lambda x: x == '-1').any(axis=1)]['Class'].value_counts())
    #In [6]

    np.random.seed(42)
    #In [7]
    df.drop(columns=['FileName'],inplace=True)
    # Index(['PdfSize', 'MetadataSize', 'Pages', 'XrefLength', 'TitleCharacters',
    #        'isEncrypted', 'EmbeddedFiles', 'Images', 'Text', 'Header', 'Obj',
    #        'Endobj', 'Stream', 'Endstream', 'Xref', 'Trailer', 'StartXref',
    #        'PageNo', 'Encrypt', 'ObjStm', 'JS', 'Javascript', 'AA', 'OpenAction',
    #        'Acroform', 'JBIG2Decode', 'RichMedia', 'Launch', 'EmbeddedFile', 'XFA',
    #        'Colors', 'Class'],
    #In [8]
    print("#Out [8]")
    print(df.columns)
    #In [9]
    target='Class'
    #in [10]
    print("Out[10]")
    print(df[target].value_counts())
    df[target]= df[target].astype('object')
    df.loc[df[target] != 'Benign', target] = 1
    df.loc[df[target] == 'Benign', target] = 0
    print(df[target].value_counts())
    df[target]= df[target].astype(dtype=np.int32)
    #In [11]
    print("Out[11]")
    cats = list(df.select_dtypes(include='category').columns)
    conts = list(df.columns.difference([target]+cats))
    print((len(cats), len(conts)))
    #In [12]
    def xs_y(df_, targ):    
        if not isinstance(targ, list):
            xs = df_[df_.columns.difference([targ])].copy()
        else:
            xs = df_[df_.columns.difference(targ)].copy()
        y = df_[targ].copy()
        return xs, y
    #in [13]
    print("Out [13]")    
    df_train=df.sample(frac=0.2,replace=False)
    df_test=df.drop(index=df_train.index)
    print((df_train.shape , df_test.shape))
    #in [14]
    df_train[cats] = df_train[cats].apply(lambda x: x.cat.codes)
    df_test[cats] = df_test[cats].apply(lambda x: x.cat.codes)
    #In [15]
    X_train, y_train = xs_y(df_train, targ=target)
    X_test, y_test = xs_y(df_test, targ=target)
    #In [16]
    def evaluate_one_feature(feature, index='', metric=roc_auc_score):    
        rootnode = DecisionTreeClassifier(max_depth=1, criterion='gini')    
        rootnode.fit(X_train[feature].array.reshape(-1,1), y_train)    
        preds = rootnode.predict(X_test[feature].array.reshape(-1,1))
        preds_tr = rootnode.predict(X_train[feature].array.reshape(-1,1))    
        met = round(metric(y_test, preds), 4)
        if met > 0.5:
            return [feature, met, rootnode, preds, preds_tr]
        else:
            return [feature, met, None, [], []]
    #In [17]
    results=parallel(f=evaluate_one_feature, 
                  items=conts+cats, n_workers=cpu_count(), threadpool=False, progress=True)
    #In [18]
    result_df = pd.DataFrame(data=results, columns=['feature', 'roc_auc_score', 'fitted_models', 'predictions', 'preds_train']).sort_values(by='roc_auc_score', ascending=False)
    #In [19]
    print("#Out [19]")
    print(result_df[['feature', 'roc_auc_score']].head(15))
    #In [22]:
    fig, axes = plt.subplots(3,3, figsize=(12,12))
    axes = axes.flatten()
    for i, tf in enumerate(result_df['feature'].head(9)):    
        sns.histplot(data=df_train, x=tf, stat='percent', hue='Class', bins=100, log_scale=(False,True), ax=axes[i])
        axes[i].tick_params(axis='x', rotation=60)
    #In [23]
    print("#Out [23]")
    useful_features = result_df.loc[result_df['roc_auc_score'] > 0.5]
    print(f"{len(useful_features)} / {len(conts+cats)} features have direct separating power (linear)")
    #In [24]
    print("#out [24]")
    ensemble_preds = np.mean(np.vstack(useful_features['predictions'].to_numpy()), axis=0)
    print(ensemble_preds.shape)
    #In [25]
    print("#Out [25]")
    ensemble_preds_train = np.mean(np.vstack(useful_features['preds_train'].to_numpy()), axis=0)
    print(ensemble_preds_train.shape)
    #In [26]
    print("#Out [26]")
    fpr, tpr, thresholds = roc_curve(y_train, ensemble_preds_train)
    # get the best threshold
    J = tpr - fpr
    ix = np.argmax(J)
    best_thresh = thresholds[ix]
    print("Best threshold", best_thresh)
    #In [27]
    print("#Out [27]")
    print("The Ensemble OneR model (simple average)")
    print("ROC-AUC", round(roc_auc_score(y_true=y_test, y_score=ensemble_preds),4))
    print("Precision", round(precision_score(y_true=y_test, y_pred=np.where(ensemble_preds >= best_thresh, 1, 0)), 4))
    print("Recall", round(recall_score(y_true=y_test, y_pred=np.where(ensemble_preds >= best_thresh, 1, 0)), 4))
    print("F1", round(f1_score(y_true=y_test, y_pred=np.where(ensemble_preds >= best_thresh, 1, 0)), 4))

    

